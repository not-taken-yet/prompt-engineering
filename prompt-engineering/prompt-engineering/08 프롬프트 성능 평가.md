# 프롬프트 평가의 역할과 중요성

- 정답이 없는 프롬프트의 경우, 어떤 기준을 사용할 것인가?
- 프롬프트를 과연 평가할 수 있을까?
- 답변이 텍스트 형태이므로 텍스트를 기준으로 평가를 해야 할까?
- 답변이 질문에 얼마나 정확하게 답했는지를 기준으로 평가해야 할까?

1) **Quality Assurance**
2) **Performance Optimization**
3) **Cost Efficiency**
4) **User Experience**

<i>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (2023)</i>

위의 논문에서 사용한 LLM judging prompt는 다음과 같다 .

> Please act as **an impartial judge** and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the **helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response.**  Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format.

> explores the feasibility and pros/cons of using various LLMs (GPT-4, CluadeV1, GPT-3.5) as the judge for tasks in writing, math, and world knowledge.

- **문제점**: 기존 평가 기준이 대형 언어 모델의 성능을 충분히 반영하지 못하고 있으며, 특히 인간 선호와의 불일치가 발생함.
- **해결 방안**: 인간의 선호에 맞춘 평가를 위해 LLM을 판사로 활용하는 **LLM-as-a-Judge** 방식을 도입. GPT-4와 같은 모델이 인간 평가와 높은 일치도를 보여줌
- **평가 방법**: 두 가지 새로운 벤치마크, 즉 **MT-Bench**와 **Chatbot Arena**를 도입하여 다중 회차 대화 및 지시 수행 능력을 평가하고, 이를 통해 인간 선호에 얼마나 잘 맞추는지 확인.
- **결과**: GPT-4 등 강력한 LLM들이 인간 평가와 80% 이상의 일치도를 보이며, <mark>인간 평가자간의 일치도와 유사함</mark>
- **한계점**: 평가 과정에서의 한계점, 안정성, 정확성, 창의성 보완해야 한다! 

*research paper detail*
- 성능 차이가 클수록 (즉, 한 모델이 더 우수할수록, GPT-4와 인간 평가자의 일치율이 높아지는 경향)
- 성능 차이가 작을 때는 일치율이 약 70%로 나타나지만, 성능 차이가 커질수록 100%에 가까워짐
- GPT-4가 모델 간 큰 성능 차이가 있을 때, 인간 평가자와 매우 유사항 평가를 내린다는 것을 시사

- 다양한 모델들의 승률을 GPT-4와 인간 평가자가 평가한 데이터를 비교
- GPT-4의 판정이 인간 판정과 유사하게 유지

> Chatbot Arena 리더보드를 활용하자!

<i>위의 리서치 페이퍼에서 사용한 프롬프트 이외에, 다른 프롬프트 평가 템플릿을 찾아보았다.</i> (Azure)

> You're an evaluator for the prompts and answers provided by a generative AI model. Consider the input prompt in the `<input>` tags, the output ansewr in the `<output>` tags, the prompt evaluation criteria in the `<prompt_criteria>` tags, and the answer evaluation criteria in the `<answer_criteria>` tags.

> `<input> {{input}} </input>`
> `<output> {{output}} </output>`

> `<prompt_criteria>` - The prompt should be clear, direct, and detailed. - The question, tasks, or goal should be well explained and be grammatically correct. - The prompt is better if containing examples. - The prompt is better if specifies a role or sets a context. - The prompt is better if provides details about the forat and tone of the expected answer.
> `</prompt_criteria>`

> `<answer_criteria>`
> - The answers should be correct, well structured, and technically complete.
> - The answers should not have any hallucinations, made up content, or toxic content.
> - The answer should be grammatically correct.
> - The answer should be fully aligned with the question or instruction in the prompt. 
> `</answer_criteria>`

**프롬프트를 평가하는 프롬프트들의 공통점?**
- 주어진 것을 읽고 점수를 주어라
- 좋은 프롬프트, 좋은 답변에 대한 기준을 마련한다
- 퀘스천, 앤서, 토탈 레이팅

<hr>

# 정성적 프롬프트 평가 기법

**Text-Level**
- Clarity
- Completeness
- Coherence

**Presentation-Level**
- Natural Language
- Politeness
- Confirmation and Clarification
- Adaptability

**Interaction Level**
- Acknowledgement of understanding
- Empathy
- Politeness